<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no" />

    <!--Update this for shortest possible description-->
    <meta name="description"
        content="Generative Augmentation Driven Prediction of Diverse Visual Scanpaths in Images." />

    <!--This part is to give credit to the person who developed the site-->
    <meta name="author" content="Aniket Patra" />

    <!---------------------og tags are for open graph ----------------->
    <meta property="og:title" content="Diverse-Visual-Scanpath" />

    <meta property="og:url" content="https://ashishverma03.github.io/diverse-visual-scanpath" />
    <meta property="og:image" content="https://ashishverma03.github.io/assets/images/lfsfa_thumb.jpg" />
    <meta property="og:image:type" content="image/jpg" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />
    <meta property="og:image:alt" content="Iterative Dehazing Training Stage 3" />

    <meta property="og:type" content="article" />
    <meta property="og:description"
        content="Generative Augmentation Driven Prediction of Diverse Visual Scanpaths in Images." />

    <meta property="og:locale" content="en_IN" />
    <meta property="og:locale:alternate" content="en_US" />

    <!---------------TWITTER CRDS------------------------>
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Diverse-Visual-Scanpath" />
    <meta name="twitter:description"
        content="Generative Augmentation Driven Prediction of Diverse Visual Scanpaths in Images." />
    <meta name="twitter:url" content="https://ashishverma03.github.io/diverse-visual-scanpath" />
    <meta name="twitter:image" content="https://ashishverma03.github.io/assets/images/lfsafa_thumb.jpg" />

    <title>Diverse Visual Scanpaths</title>

    <!--Page Icon-->
    <link rel="icon" type="image/png" href="assets/images/ak2.ico" />

    <!--STYLESHEETS (.css)-->

    <!--For Bootstrap 5-->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">

    <!--Extrenal css-->
    <link rel="stylesheet" href="diverse-visual-scanpath/subpage.css" />
</head>

<body>
    <div class="container" style="background-color: rgb(255, 255, 255)">
        <!--Top Nav-->
        <header>
            <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
                <div class="container-fluid">
                    <a class="navbar-brand" href="https://ashishverma03.github.io/"><i
                            class="fas fa-house-user d-inline-block"></i></a>
                    <button class="navbar-toggler" type="button" data-bs-toggle="collapse"
                        data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent"
                        aria-expanded="false" aria-label="Toggle navigation">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <div class="collapse navbar-collapse" id="navbarSupportedContent">
                        <ul class="navbar-nav me-auto mb-2 mb-lg-0">
                            <li class="nav-item">
                                <a class="nav-link" href="#sectionResult">Result</a>
                            </li>
                            <li class="nav-item">
                                <a class="nav-link" href="#download">Download</a>
                            </li>
                        </ul>
                    </div>
                </div>
            </nav>
        </header>

        <!--Topic Name-->
        <h1 style="
          text-align: center;
          font-size: calc(25px + 0.5vw);
          font-family: 'Raleway', sans-serif;
          font-weight: bold;
        ">
            Generative Augmentation Driven Prediction of Diverse Visual Scanpaths in Images
        </h1>

        <!--Author Name-->

        <h2 style="
          text-align: center;
          font-size: calc(10px + 0.5vw);
          font-family: 'Roboto', sans-serif;
        ">
            <a href="https://github.com/ashishverma03" target="_blank" style="text-decoration: none; color: rgb(70, 70, 70)"
                onmouseover="this.style.color='gray'" onmouseout="this.style.color='rgb(70, 70, 70)'"><u>Ashish
                    Verma</u></a>, Debashis Sen
        </h2>

        <!--Institution Name-->

        <h3 style="
          text-align: center;
          font-size: calc(10px + 0.5vw);
          font-family: 'Roboto', sans-serif;
        ">
            Department of Electronics and Electrical Communication Engineering
            <br />
            Indian Institute of Technology Kharagpur, India
        </h3>
        <hr />
        <!------------------------------ABSTRACT-------------------->
        <h2 id="headingStyles" class="text-center text-md-start">
            <i class="fas fa-lightbulb"></i>&nbsp;Abstract
        </h2>
        <div class="row">
            <div class="col-sm">
                <p style="text-align: justify">
                    Visual scanpaths of multiple humans on an image represent the process
                    by which they capture the information in it. State-of-the-art models 
                    to predict visual scanpaths on images learn directly from recorded 
                    human visual scanpaths. However, the generation of multiple visual
                    scanpaths on an image having diversity like human visual scanpaths
                    has not been explicitly considered. In this paper, we propose a deep
                    network for predicting multiple diverse visual scanpaths on an image.
                    Image-specific hidden Markov model based generative data augmentation
                    is performed in the beginning to increase the number of image-visual
                    scanpath training pairs. Considering a similarity between our generative
                    data augmentation process and the use of long short-term memory (LSTM)
                    for prediction, we propose an LSTM based visual scanpath predictor.
                    A network to predict a single visual scanpath on an image is designed first.
                    The network is then modified to predict multiple diverse visual scanpaths
                    representing different viewer varieties by using a parameter indicating the
                    uniqueness of a viewer. A random vector is also employed for subtle variations
                    within scanpaths of the same viewer variety. Our models are evaluated on three
                    standard datasets using multiple performance measures, which demonstrate the
                    superiority of the proposed approach over the state-of-the-art. Empirical
                    studies are also given indicating the significance of our generative data
                    augmentation method and our multiple scanpath prediction strategy producing
                    diverse visual scanpaths.
                </p>
            </div>
        </div>
        <hr>
        <!------------------------------HIGHLIGHTS-------------------->
        <h2 id="headingStyles" class="text-center text-md-start">
            <i class="fas fa-satellite-dish"></i>&nbsp;Highlights
        </h2>
        <div class="row" id="jumbo">
            <div class="col">
                <ol type="i">
                    <li>
                        A visual scanpath predictor network for images, which is
                        trained end-to-end, driven by generative data augmentation.
                    </li>
                    <li>
                        A HMM-based generative data augmentation procedure to obtain
                        image-specific training pairs of images & visual scanpaths.
                    </li>
                    <li>
                        A training strategy based on the uniqueness of a viewer to 
                        generate multiple and diverse visual scanpaths of different 
                        viewer varieties for an image.
                    </li>
                </ol>
            </div>
        </div>
        <hr />
        <!------------------------------Proposed module-------------------->
        <h2 id="headingStyles" class="text-center text-md-start">
            <i class="fas fa-puzzle-piece"></i>&nbsp;Proposed Architecture
        </h2>
        <div class="row ">
            <!-------------------------------FIGURE 1--------------------------->
            <div class="col d-flex justify-content-center">
                <figure class="figure">
                    <span class="d-flex justify-content-center">
                        <img class="img-fluid figure-img" src="assets/subpage-diverse-visual-scanpath/archtitecture/new_overall_model.jpg"
                            class="figure-img img-fluid rounded" title="proposed architecture image"
                            height="800" width="1200"
                            alt="proposed diverse visual scanpath prediction" /></a></span>
                    <figcaption class="text-center" id="figCap">
                        The architecture of the proposed model for visual scanpath 
                        prediction comprises of two main novel components: augmentation
                        of image-visual scanpath pairs using HMM and the LSTM-based 
                        scanpath predictor.
                        <!--EDIT CAPTION HERE-->
                    </figcaption>
                </figure>
            </div>
        </div>
        <hr>
        <!------------------------------Results and ABalation Studies-------------------->
        <div class="col" id="sectionResult">
            <!-------------------------------FIGURE 2--------------------------->
            <div class="col d-flex justify-content-center">
                <h2 id="headingStyles" class="text-center text-md-start">
                    <i class="fas fa-poll"></i>&nbsp;Sample Results (Full results will be released soon)
                </h2>
                <figure class="figure">
                    <span class="d-flex justify-content-center">
                        <img class="img-fluid figure-img" src="assets/subpage-diverse-visual-scanpath/Results/Table1.jpg"
                            class="figure-img img-fluid rounded" title="Proposed sub-aperture feature adaptation module"
                            height="600" width="600"
                            alt="proposed diverse visual scanpath prediction" /></a></span>
                    <figcaption class="text-center" id="figCap">
                        Table 1: Performance comparison of the various multiple scanpath prediction models on 
                        the OSIE test set. The top-3 ranked models for each MultiMatch score are indicated by 
                        subscripts. The top-3 models with the closest Intra-SS to Human-GT's (G) are also 
                        indicated by subscripts to tick marks denoting they are within G&plusmn;0.25.
                        <!--EDIT CAPTION HERE-->
                    </figcaption>
                </figure>
            </div>
            <br>
            <div class="col d-flex justify-content-center">
                <h2 id="headingStyles" class="text-center text-md-start">
                    <i class="fas fa-poll"></i>&nbsp;
                </h2>
                <div class="row">
                    <div class="col-12">
                        <figure class="figure">
                            <span class="d-flex justify-content-center">
                                <img class="img-fluid figure-img" src="assets/subpage-diverse-visual-scanpath/Results/Table2.jpg"
                                    class="figure-img img-fluid rounded" title="Model ablation studies" height="600"
                                    width="1200" alt="Table number 2, caption is mentioned below." /></a></span>
                            <figcaption class="text-center" id="figCap">
                                Table 2: Cross-dataset evaluation of the various multiple scanpath prediction models.
                                The top-3 ranked models for each MultiMatch score are indicated by subscripts. 
                                The top-3 models with the closest Intra-SS to that of Human-GT (G) are also indicated
                                by subscripts to tick marks denoting that they are within G&plusmn;0.25.
                                <!--EDIT CAPTION HERE-->
                            </figcaption>
                        </figure>
                    </div>  
                </div>
            </div>
        </div>
        <hr>
        <!------------------------------Visual Comparison-------------------->
        <h2 id="headingStyles" class="text-center text-md-start">
            <i class="fas fa-eye"></i>&nbsp;Visual Comparison
        </h2>
        <div class="row ">
            <!-------------------------------FIGURE 2--------------------------->
            <div class="col d-flex justify-content-center">
                <figure class="figure">
                    <span class="d-flex justify-content-center">
                        <img class="img-fluid figure-img" src="assets/subpage-diverse-visual-scanpath/visComparison/Final_plotted_image1.jpg"
                            class="figure-img img-fluid rounded"
                            title="Qualitative comparison of our proposed DiviScan" width="1300"
                            alt="Image for qualitative comparison of our proposed DiviScan" /></a></span>
                    <figcaption class="text-center" id="figCap">
                        Visual scanpaths (with highest SS) predicted by the various models embedded on images with different numbers of objects of interest.
                        <!--EDIT CAPTION HERE-->
                    </figcaption>
                </figure>
            </div>
        </div>
        <hr>
        <!-----------------------DOWNLOADS------------------------------->
        <h2 id="download" class="text-center text-md-start">
            <i class="fas fa-download"></i>&nbsp;Download
        </h2>
        <div class="row">
            <div class="col-sm-4 text-center">
                <figure class="figure">
                    <a href="https://github.com/ashishverma03/DiViScan" rel="external nofollow" target="_blank"
                        onmouseover="this.style.opacity='0.5'" onmouseout="this.style.opacity='1'"><i
                            class="fab fa-github" style="font-size: 120px; color: rgb(54, 54, 54)"></i>
                    </a>
                    <figcaption class="text-center figure-caption" style="font-size: calc(12px + 0.5vw)">
                        Code
                    </figcaption>
                </figure>
            </div>
            <div class="col-sm-4 text-center">
                <figure class="figure">
                    <a href="#########ENTER_LINK###########" rel="external nofollow" target="_blank"
                        onmouseover="this.style.opacity='0.5'" onmouseout="this.style.opacity='1'"><img
                            src="assets/images/gdrive.png" alt="Google drive image icon" width="150px" height="120px"
                            title="Click to Open" />
                    </a>
                    <figcaption class="text-center figure-caption" style="font-size: calc(12px + 0.5vw)">
                        Training & Testing Datasets
                    </figcaption>
                </figure>
            </div>
        </div>
        <hr />
        <!-------------------------REFERENCES---------------------------->
        <h2 id="headingStyles" class="text-center text-md-start">
            <i class="fas fa-asterisk"></i>&nbsp;References
        </h2>
        <div class="row">
            <div class="col">
                <ul>
                    <li style="text-align: justify">
                        [7] L. Itti, C. Koch, and E. Niebur, “A model of saliency-based visual attention for rapid scene analysis,” IEEE T-PAMI, no. 11, pp. 1254–1259, Nov. 1998.
                    </li>
                    <li style="text-align: justify">
                        [10] O. Le Meur and Z. Liu, “Saccadic model of eye movements for free-viewing condition,” Vis. Res., vol. 116, pp. 152–164, Nov. 2015.
                    </li>
                    <li style="text-align: justify">
                        [11] C. Wloka, I. Kotseruba, and J. K. Tsotsos, “Active fixation control to predict saccade sequences,” in CVPR, Jun. 2018, pp. 3184–3193.
                    </li>
                    <li style="text-align: justify">
                        [13] W. Sun, Z. Chen, and F. Wu, “Visual scanpath prediction using IOR-ROI recurrent mixture density network,” IEEE T-PAMI, vol. 43, no. 6, pp. 2101–2118, Dec. 2019.
                    </li>
                    <li style="text-align: justify">
                        [22] G. Boccignone and M. Ferraro, “Modelling gaze shift as a constrained random walk,” Phys. A, Statist. Mech. Appl., vol. 331, no. 1-2, pp. 207– 218, Jan. 2004.19.
                    </li>
                    <li style="text-align: justify">
                        [28] M. Assens Reina, X. Giro-i Nieto, K. McGuinness, and N. E. O’Connor, “Saltinet: Scan-path prediction on 360 degree images using saliency volumes,” in ICCVW, Oct. 2017, pp. 2331–2338.
                    </li>
                    <li style="text-align: justify">
                        [29] M. Assens, X. Giro-i Nieto, K. McGuinness, and N. E. O’Connor, “Path-GAN: Visual scanpath prediction with generative adversarial networks,” in ECCVW, Sep. 2018.
                    </li>
                    <li style="text-align: justify">
                        [33] X. Chen, M. Jiang, and Q. Zhao, “Predicting human scanpaths in visual question answering,” in CVPR, Jun. 2021, pp. 10 876–10 885.
                    </li>
                </ul>
            </div>
        </div>
        <hr />

        <!-----------------------------FOOTER------------------------------->
        <footer>
            <p style="text-align: center">
                <i class="far fa-copyright"></i> 2022
                <a href="https://github.com/aupendu" target="_blank">Ashish Verma</a>.
                Made in
                <a href="https://www.incredibleindia.org/" target="_blank"><img id="flag"
                        src="/assets/images/india-flag-icon-32.png" alt="My Great Country India's Flag" /></a>
                by
                <a href="https://thingsbypatra.pythonanywhere.com//" target="_blank">Patra
                </a>
            </p>
        </footer>
    </div>

    <!--SCRIPTS (.js)-->
    <!--JQUERY-->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"
        integrity="sha512-894YE6QWD5I59HgZOGReFYm4dnWc1Qt5NtvYSaNcOP+u1T9qYdvdihz0PPSiiqn/+/3e7Jo4EaG7TubfWGUrMQ=="
        crossorigin="anonymous" referrerpolicy="no-referrer"></script>

    <!--Font Awesome 5-->
    <script src="https://kit.fontawesome.com/58bac38d13.js" crossorigin="anonymous"></script>

    <!--Bootstrap 5-->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM"
        crossorigin="anonymous"></script>

    <!--External JS-->
    <script src="lfsafa-sr/subjs.js"></script>
</body>

</html>

<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no" />

    <!--Update this for shortest possible description-->
    <meta name="description"
        content="RGB-D Fusion through Zero-shot Fuzzy Membership Learning for Salient Object Detection." />

    <!--This part is to give credit to the person who developed the site-->
    <meta name="author" content="Aniket Patra" />

    <!---------------------og tags are for open graph ----------------->
    <meta property="og:title" content="Fuzzy-Fusion-SOD" />

    <meta property="og:url" content="https://https://sudiptabhuyan1.github.io//Fuzzy-Fusion-SOD" />
    <meta property="og:image" content="https://https://sudiptabhuyan1.github.io//assets/images/lfsfa_thumb.jpg" />
    <meta property="og:image:type" content="image/jpg" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />
    <meta property="og:image:alt" content="" />

    <meta property="og:type" content="article" />
    <meta property="og:description"
        content="RGB-D Fusion through Zero-shot Fuzzy Membership Learning for Salient Object Detection." />

    <meta property="og:locale" content="en_IN" />
    <meta property="og:locale:alternate" content="en_US" />

    <!---------------TWITTER CRDS------------------------>
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Fuzzy-Fusion-SOD" />
    <meta name="twitter:description"
        content="RGB-D Fusion through Zero-shot Fuzzy Membership Learning for Salient Object Detection." />
    <meta name="twitter:url" content="https://sudiptabhuyan1.github.io/Fuzzy-Fusion-SOD" />
    <meta name="twitter:image" content="https://sudiptabhuyan1.github.io/assets/images/lfsafa_thumb.jpg" />

    <title>Fuzzy-Fusion-SOD</title>

    <!--Page Icon-->
    <link rel="icon" type="image/png" href="assets/images/ak2.ico" />

    <!--STYLESHEETS (.css)-->

    <!--For Bootstrap 5-->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">

    <!--Extrenal css-->
    <link rel="stylesheet" href="subpage-fuzzy-fusion-SOD/subpage.css" />
</head>

<body>
    <div class="container" style="background-color: rgb(255, 255, 255)">
        <!--Top Nav-->
        <header>
            <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
                <div class="container-fluid">
                    <a class="navbar-brand" href="https://sudiptabhuyan1.github.io/"><i
                            class="fas fa-house-user d-inline-block"></i></a>
                    <button class="navbar-toggler" type="button" data-bs-toggle="collapse"
                        data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent"
                        aria-expanded="false" aria-label="Toggle navigation">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <div class="collapse navbar-collapse" id="navbarSupportedContent">
                        <ul class="navbar-nav me-auto mb-2 mb-lg-0">
                            <li class="nav-item">
                                <a class="nav-link" href="#sectionResult">Result</a>
                            </li>
                            <li class="nav-item">
                                <a class="nav-link" href="#download">Download</a>
                            </li>
                        </ul>
                    </div>
                </div>
            </nav>
        </header>

        <!--Topic Name-->
        <h1 style="
          text-align: center;
          font-size: calc(25px + 0.5vw);
          font-family: 'Raleway', sans-serif;
          font-weight: bold;
        ">
            RGB-D Fusion through Zero-shot Fuzzy Membership Learning for Salient Object Detection
        </h1>

        <!--Author Name-->

        <h2 style="
          text-align: center;
          font-size: calc(10px + 0.5vw);
          font-family: 'Roboto', sans-serif;
        ">
            <a href="https://github.com/sudiptabhuyan1" target="_blank" style="text-decoration: none; color: rgb(70, 70, 70)"
                onmouseover="this.style.color='gray'" onmouseout="this.style.color='rgb(70, 70, 70)'"><u>Sudipta Bhuyan
                    </u></a>, Aupendu Kar  </u></a>, Debashis Sen </u></a>, Sankha Deb
        </h2>

        <!--Institution Name-->

        <h3 style="
          text-align: center;
          font-size: calc(10px + 0.5vw);
          font-family: 'Roboto', sans-serif;
        ">
            Advanced Technology Development Centre
            <br />
            Indian Institute of Technology Kharagpur, India
        </h3>
        <hr />
        <!------------------------------ABSTRACT-------------------->
        <h2 id="headingStyles" class="text-center text-md-start">
            <i class="fas fa-lightbulb"></i>&nbsp;Abstract
        </h2>
        <div class="row">
            <div class="col-sm">
                <p style="text-align: justify">
                    Significant improvement has been achieved lately
                    in color and depth data based salient object detection on images
                    from varied datasets, which is mainly due to RGB-D fusion using
                    modern machine learning techniques. However, little emphasis
                    has been given recently on performing RGB-D fusion for salient
                    object detection in the absence of ground truth data for training.
                    This paper proposes a zero-shot deep RGB-D fusion approach
                    based on the novel concept of fuzzy membership learning, which
                    does not require any paired or unpaired data for training.
                    The constituent salient object maps to be fused are represented
                    using parametric fuzzy membership functions and the optimal
                    parameter values are estimated through our zero-shot fuzzy
                    membership learning (Z-FML) network. The optimal parameter
                    values are used in a fuzzy inference system along with the
                    constituent salient object maps to perform the fusion. A measure
                    called the membership similarity measure (MSM) is proposed,
                    and the Z-FML network is trained using it to devise a loss
                    function that maximizes the similarity between the constituent
                    salient object maps and the fused salient object map. The
                    deduction of MSM and its properties are shown theoretically, and
                    the gradients involved in the training of the Z-FML network are
                    derived. Qualitative and quantitative evaluations using several
                    datasets signify the effectiveness of our RGB-D fusion and our
                    fusion based RGB-D salient object detection in comparison to the
                    state-of-the-art. We also empirically demonstrate the advantage
                    of employing the novel MSM for training our Z-FML network.
                </p>
            </div>
        </div>
        <hr>
        <!------------------------------HIGHLIGHTS-------------------->
        <h2 id="headingStyles" class="text-center text-md-start">
            <i class="fas fa-satellite-dish"></i>&nbsp;Highlights
        </h2>
        <div class="row" id="jumbo">
            <div class="col">
                <ol type="i">
                    <li>
                        We propose a zero-shot framework for deep RGB-D
                        fusion leading to salient object detection, which can
                        work with arbitrary color and depth processing modules
                        without any prior training.
                    </li>
                    <li>
                        We propose a measure that quantifies similarity between
                        fuzzy memberships and state how its properties are favorable
                        for our purpose.
                    </li>
                    <li>
                        To the best of our knowledge, ours is the first work on
                        fuzzy membership learning through zero-shot network
                        optimization.
                    </li>
                </ol>
            </div>
        </div>
        <hr />
        <!------------------------------Proposed module-------------------->
        <h2 id="headingStyles" class="text-center text-md-start">
            <i class="fas fa-puzzle-piece"></i>&nbsp;Proposed Architecture
        </h2>
        <div class="row ">
            <!-------------------------------FIGURE 1--------------------------->
            <div class="col d-flex justify-content-center">
                <figure class="figure">
                    <span class="d-flex justify-content-center">
                        <img class="img-fluid figure-img" src="assets/subpage-fuzzy-fusion-SOD/archtitecture/new_overall_model.jpg"
                            class="figure-img img-fluid rounded" title="proposed architecture image"
                            height="800" width="1200"
                            alt="proposed diverse visual scanpath prediction" /></a></span>
                    <figcaption class="text-center" id="figCap">
                        The architecture of the proposed model for visual scanpath 
                        prediction comprises of two main novel components: augmentation
                        of image-visual scanpath pairs using HMM and the LSTM-based 
                        scanpath predictor.
                        <!--EDIT CAPTION HERE-->
                    </figcaption>
                </figure>
            </div>
        </div>
        <hr>
        <!------------------------------Results and ABalation Studies-------------------->
        <div class="col" id="sectionResult">
            <!-------------------------------FIGURE 2--------------------------->
            <div class="col d-flex justify-content-center">
                <h2 id="headingStyles" class="text-center text-md-start">
                    <i class="fas fa-poll"></i>&nbsp;Sample Results (Full results will be released soon)
                </h2>
                <figure class="figure">
                    <span class="d-flex justify-content-center">
                        <img class="img-fluid figure-img" src="assets/subpage-diverse-visual-scanpath/Results/Table1.jpg"
                            class="figure-img img-fluid rounded" title="Proposed sub-aperture feature adaptation module"
                            height="600" width="600"
                            alt="proposed diverse visual scanpath prediction" /></a></span>
                    <figcaption class="text-center" id="figCap">
                        Table 1: Performance comparison of the various multiple scanpath prediction models on 
                        the OSIE test set. The top-3 ranked models for each MultiMatch score are indicated by 
                        subscripts. The top-3 models with the closest Intra-SS to Human-GT's (G) are also 
                        indicated by subscripts to tick marks denoting they are within G&plusmn;0.25.
                        <!--EDIT CAPTION HERE-->
                    </figcaption>
                </figure>
            </div>
            <br>
            <div class="col d-flex justify-content-center">
                <h2 id="headingStyles" class="text-center text-md-start">
                    <i class="fas fa-poll"></i>&nbsp;
                </h2>
                <div class="row">
                    <div class="col-12">
                        <figure class="figure">
                            <span class="d-flex justify-content-center">
                                <img class="img-fluid figure-img" src="assets/subpage-diverse-visual-scanpath/Results/Table2.jpg"
                                    class="figure-img img-fluid rounded" title="Model ablation studies" height="600"
                                    width="1200" alt="Table number 2, caption is mentioned below." /></a></span>
                            <figcaption class="text-center" id="figCap">
                                Table 2: Cross-dataset evaluation of the various multiple scanpath prediction models.
                                The top-3 ranked models for each MultiMatch score are indicated by subscripts. 
                                The top-3 models with the closest Intra-SS to that of Human-GT (G) are also indicated
                                by subscripts to tick marks denoting that they are within G&plusmn;0.25.
                                <!--EDIT CAPTION HERE-->
                            </figcaption>
                        </figure>
                    </div>  
                </div>
            </div>
        </div>
        <hr>
        <!------------------------------Visual Comparison-------------------->
        <h2 id="headingStyles" class="text-center text-md-start">
            <i class="fas fa-eye"></i>&nbsp;Visual Comparison
        </h2>
        <div class="row ">
            <!-------------------------------FIGURE 2--------------------------->
            <div class="col d-flex justify-content-center">
                <figure class="figure">
                    <span class="d-flex justify-content-center">
                        <img class="img-fluid figure-img" src="assets/subpage-diverse-visual-scanpath/visComparison/Final_plotted_image1.jpg"
                            class="figure-img img-fluid rounded"
                            title="Qualitative comparison of our proposed DiviScan" width="1300"
                            alt="Image for qualitative comparison of our proposed DiviScan" /></a></span>
                    <figcaption class="text-center" id="figCap">
                        Visual scanpaths (with highest SS) predicted by the various models embedded on images with different numbers of objects of interest.
                        <!--EDIT CAPTION HERE-->
                    </figcaption>
                </figure>
            </div>
        </div>
        <hr>
        <!-----------------------DOWNLOADS------------------------------->
        <h2 id="download" class="text-center text-md-start">
            <i class="fas fa-download"></i>&nbsp;Download
        </h2>
        <div class="row">
            <div class="col-sm-4 text-center">
                <figure class="figure">
                    <a href="https://github.com/ashishverma03/DiViScan" rel="external nofollow" target="_blank"
                        onmouseover="this.style.opacity='0.5'" onmouseout="this.style.opacity='1'"><i
                            class="fab fa-github" style="font-size: 120px; color: rgb(54, 54, 54)"></i>
                    </a>
                    <figcaption class="text-center figure-caption" style="font-size: calc(12px + 0.5vw)">
                        Code
                    </figcaption>
                </figure>
            </div>
            <div class="col-sm-4 text-center">
                <figure class="figure">
                    <a href="#########ENTER_LINK###########" rel="external nofollow" target="_blank"
                        onmouseover="this.style.opacity='0.5'" onmouseout="this.style.opacity='1'"><img
                            src="assets/images/gdrive.png" alt="Google drive image icon" width="150px" height="120px"
                            title="Click to Open" />
                    </a>
                    <figcaption class="text-center figure-caption" style="font-size: calc(12px + 0.5vw)">
                        Training & Testing Datasets
                    </figcaption>
                </figure>
            </div>
        </div>
        <hr />
        <!-------------------------REFERENCES---------------------------->
        <h2 id="headingStyles" class="text-center text-md-start">
            <i class="fas fa-asterisk"></i>&nbsp;References
        </h2>
        <div class="row">
            <div class="col">
                <ul>
                    <li style="text-align: justify">
                        [7] L. Itti, C. Koch, and E. Niebur, “A model of saliency-based visual attention for rapid scene analysis,” IEEE T-PAMI, no. 11, pp. 1254–1259, Nov. 1998.
                    </li>
                    <li style="text-align: justify">
                        [10] O. Le Meur and Z. Liu, “Saccadic model of eye movements for free-viewing condition,” Vis. Res., vol. 116, pp. 152–164, Nov. 2015.
                    </li>
                    <li style="text-align: justify">
                        [11] C. Wloka, I. Kotseruba, and J. K. Tsotsos, “Active fixation control to predict saccade sequences,” in CVPR, Jun. 2018, pp. 3184–3193.
                    </li>
                    <li style="text-align: justify">
                        [13] W. Sun, Z. Chen, and F. Wu, “Visual scanpath prediction using IOR-ROI recurrent mixture density network,” IEEE T-PAMI, vol. 43, no. 6, pp. 2101–2118, Dec. 2019.
                    </li>
                    <li style="text-align: justify">
                        [22] G. Boccignone and M. Ferraro, “Modelling gaze shift as a constrained random walk,” Phys. A, Statist. Mech. Appl., vol. 331, no. 1-2, pp. 207– 218, Jan. 2004.19.
                    </li>
                    <li style="text-align: justify">
                        [28] M. Assens Reina, X. Giro-i Nieto, K. McGuinness, and N. E. O’Connor, “Saltinet: Scan-path prediction on 360 degree images using saliency volumes,” in ICCVW, Oct. 2017, pp. 2331–2338.
                    </li>
                    <li style="text-align: justify">
                        [29] M. Assens, X. Giro-i Nieto, K. McGuinness, and N. E. O’Connor, “Path-GAN: Visual scanpath prediction with generative adversarial networks,” in ECCVW, Sep. 2018.
                    </li>
                    <li style="text-align: justify">
                        [33] X. Chen, M. Jiang, and Q. Zhao, “Predicting human scanpaths in visual question answering,” in CVPR, Jun. 2021, pp. 10 876–10 885.
                    </li>
                </ul>
            </div>
        </div>
        <hr />

        <!-----------------------------FOOTER------------------------------->
        <footer>
            <p style="text-align: center">
                <i class="far fa-copyright"></i> 2022
                <a href="https://github.com/aupendu" target="_blank">Ashish Verma</a>.
                Made in
                <a href="https://www.incredibleindia.org/" target="_blank"><img id="flag"
                        src="/assets/images/india-flag-icon-32.png" alt="My Great Country India's Flag" /></a>
                by
                <a href="https://thingsbypatra.pythonanywhere.com//" target="_blank">Patra
                </a>
            </p>
        </footer>
    </div>

    <!--SCRIPTS (.js)-->
    <!--JQUERY-->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"
        integrity="sha512-894YE6QWD5I59HgZOGReFYm4dnWc1Qt5NtvYSaNcOP+u1T9qYdvdihz0PPSiiqn/+/3e7Jo4EaG7TubfWGUrMQ=="
        crossorigin="anonymous" referrerpolicy="no-referrer"></script>

    <!--Font Awesome 5-->
    <script src="https://kit.fontawesome.com/58bac38d13.js" crossorigin="anonymous"></script>

    <!--Bootstrap 5-->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM"
        crossorigin="anonymous"></script>

    <!--External JS-->
    <script src="lfsafa-sr/subjs.js"></script>
</body>

</html>
